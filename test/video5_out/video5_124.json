{"opt": {"output_dir": "", "num_to_draw": 10, "final_nms_thresh": 0.3, "use_cudnn": 1, "text_size": 2, "max_images": 100, "gpu": -1, "splits_json": "info/densecap_splits.json", "vg_img_root_dir": "", "checkpoint": "data/models/densecap/densecap-pretrained-vgg16.t7", "num_proposals": 1000, "rpn_nms_thresh": 0.7, "image_size": 720, "input_image": "/home/akash/learn/598/project/video-context-transcription/test/video5/video5_124.jpg", "input_split": "", "box_width": 2, "input_dir": "", "output_vis_dir": "/home/akash/learn/598/project/video-context-transcription/src/python/default_out_dir", "output_vis": 1}, "results": [{"img_name": "video5_124.jpg", "scores": [2.3427696228027, 1.7206196784973, 1.6377038955688, 0.53639268875122], "captions": ["a man standing at a table", "woman with brown hair", "a black and silver laptop", "the face of a woman"], "boxes": [[1.4204406738281, 5.5391693115234, 701.86999511719, 389.15289306641], [44.953887939453, 1.0478591918945, 352.79510498047, 154.66369628906], [484.35150146484, 301.18609619141, 236.31420898438, 100.67956542969], [179.98538208008, 15.714225769043, 109.13357543945, 160.0608215332]], "sg": [{"relationships": [{"predicate": "stand at", "subject": 0, "text": ["man", "stand at", "table"], "object": 1}], "phrase": "", "objects": [{"names": ["man"]}, {"names": ["table"]}], "attributes": [], "id": 0, "url": "0"}, {"relationships": [{"predicate": "with", "subject": 0, "text": ["woman", "with", "hair"], "object": 1}], "phrase": "", "objects": [{"names": ["woman"]}, {"names": ["hair"]}], "attributes": [{"predicate": "is", "subject": 1, "attribute": "brown", "text": ["hair", "is", "brown"], "object": "brown"}], "id": 0, "url": "0"}, {"relationships": [], "phrase": "", "objects": [], "attributes": [], "id": 0, "url": "0"}, {"relationships": [{"predicate": "of", "subject": 0, "text": ["face", "of", "woman"], "object": 1}], "phrase": "", "objects": [{"names": ["face"]}, {"names": ["woman"]}], "attributes": [], "id": 0, "url": "0"}]}]}