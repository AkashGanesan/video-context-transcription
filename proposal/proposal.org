#+TITLE: Video Based Contextual Question Answering 
#+OPTIONS: ':nil *:t -:t ::t <:t H:3 \n:nil ^:{} arch:headline 
#+OPTIONS: author:nil c:nil creator:comment d:(not "LOGBOOK") date:t
#+OPTIONS: e:t email:nil f:t inline:t num:t p:nil pri:nil stat:t
#+OPTIONS: tags:t tasks:t tex:t timestamp:t toc:nil todo:t |:t
#+CREATOR: Emacs 25.2.2 (Org mode 8.2.10)
#+DESCRIPTION:
#+EXCLUDE_TAGS: noexport
#+KEYWORDS:
#+LANGUAGE: en
#+SELECT_TAGS: export
#+LATEX_CLASS: acmart
#+LATEX_HEADER: \usepackage{cuted}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{lmodern}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage[margin=0.5in]{geometry}

#+LATEX_HEADER: \author{Akash Ganesan}
#+LATEX_HEADER: \affiliation{%
#+LATEX_HEADER: }
#+LATEX_HEADER:  
#+LATEX_HEADER: \email{akaberto@umich.edu}
#+LATEX_HEADER:  
#+LATEX_HEADER: \author{Divyansh Pal}
#+LATEX_HEADER: \affiliation{%
#+LATEX_HEADER: }
#+LATEX_HEADER: \email{divpal@umich.edu}
#+LATEX_HEADER:  
#+LATEX_HEADER: \author{Karthik Muthuraman}
#+LATEX_HEADER: \affiliation{%
#+LATEX_HEADER: }
#+LATEX_HEADER: \email{mkarthik@umich.edu}
#+LATEX_HEADER:  
#+LATEX_HEADER: \author{Shubham Dash}
#+LATEX_HEADER: \affiliation{%
#+LATEX_HEADER: }
#+LATEX_HEADER: \email{shubhamd@umich.edu}
#+LATEX_HEADER:  

#+LATEX_HEADER: \settopmatter{printacmref=false} % Removes citation information below abstract
#+LATEX_HEADER: \renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
#+LATEX_HEADER: \pagestyle{plain} % removes running headers
#+LATEX_HEADER:  


#+LATEX_HEADER: \begin{CCSXML}
#+LATEX_HEADER: <ccs2012>
#+LATEX_HEADER: <concept>
#+LATEX_HEADER: <concept_id>10010147.10010178.10010179.10010182</concept_id>
#+LATEX_HEADER: <concept_desc>Computing methodologies~Natural language generation</concept_desc>
#+LATEX_HEADER: <concept_significance>500</concept_significance>
#+LATEX_HEADER: </concept>
#+LATEX_HEADER: <concept>
#+LATEX_HEADER: <concept_id>10010147.10010178.10010224.10010225.10010227</concept_id>
#+LATEX_HEADER: <concept_desc>Computing methodologies~Scene understanding</concept_desc>
#+LATEX_HEADER: <concept_significance>500</concept_significance>
#+LATEX_HEADER: </concept>
#+LATEX_HEADER: <concept>
#+LATEX_HEADER: <concept_id>10010147.10010178.10010224.10010245.10010250</concept_id>
#+LATEX_HEADER: <concept_desc>Computing methodologies~Object detection</concept_desc>
#+LATEX_HEADER: <concept_significance>500</concept_significance>
#+LATEX_HEADER: </concept>
#+LATEX_HEADER: </ccs2012>
#+LATEX_HEADER: <ccs2012>
#+LATEX_HEADER: <concept>
#+LATEX_HEADER: <concept_id>10010147.10010178.10010224.10010225.10010231</concept_id>
#+LATEX_HEADER: <concept_desc>Computing methodologies~Visual content-based indexing and retrieval</concept_desc>
#+LATEX_HEADER: <concept_significance>500</concept_significance>
#+LATEX_HEADER: </concept>
#+LATEX_HEADER: </ccs2012>
#+LATEX_HEADER: \end{CCSXML}

#+LATEX_HEADER: \ccsdesc[500]{Computing methodologies~Object detection}
#+LATEX_HEADER: \ccsdesc[500]{Computing methodologies~Natural language generation}
#+LATEX_HEADER: \ccsdesc[500]{Computing methodologies~Scene understanding}
#+LATEX_HEADER: \ccsdesc[500]{Computing methodologies~Visual content-based indexing and retrieval}   
#+LATEX_HEADER: 


* Problem Definition

  The problem we are primarily looking to solve aims at building a
  contextual Question-Answering model for videos. The current
  methodologies provide a robust model for image-based
  Question-Answering, but we are trying to generalize this approach to
  be videos.  The model should also be able to handle contextual
  queries across the whole video.  For example, if a frame has an
  image of a man and a cat sitting, it should be able to handle
  queries like, “where was the cat sitting with respect to the man?”
  or ,“what is the man holding in his hand?”.  It should be able to
  answer queries relating to temporal relationships.

  Initially, we shall focus on handling only yes/no questions and then
  extend to complex questions like “What?”, “How?”, “When?” and
  “Where?”.


* Challenges

  There are multiple challenges which we have to deal with in order to
  successfully build a model which can efficiently handle queries and
  answer them and also preserve the context embedded in the video.
  First challenge would be to build a contextual linking in between
  the scene graphs. Consider an example where a man is eating food in
  one frame which is considered as a key frame and in the second frame
  he is driving a car.  So, the link from one scene graph and the
  second scene graph should be linked in such a way so that the graph
  when traversed for finding out the answers should be representative
  of how to video is evolving over time.  Secondly, for gauging the
  efficiency of our proposed model, we need an evaluation metric.  One
  more interesting challenge is scalability.  Here, we use time taken
  to process the video and answer queries as the the metric.  

* Prior Work

  Previous related work can be found in the related fields of video
  summarization cite:DBLP:journals/corr/ZhangCSG16a, image captioning
  cite:k2,d1 and scene graph generation cite:k1,s2. We reviewed papers
  that propose methods to generate key-frames of interest from a long
  video. A major part of our project will draw from works relating to
  dense captioning of images and generating scene graphs. There is a
  wide body of recent literature which propose novel and optimized
  techniques for the aforementioned tasks. Most of these rely on Deep
  Learning methods for object detection and captioning and Machine
  Learning(ML) and optimization techniques to generate the scene
  graphs. Finally, in terms of evaluation processes, possible test
  datasets and possible metrics, there are papers and open datasets
  that are relevant to our project cite:DBLP:conf/cvpr/NohSH16.

* Proposed Methodology

  We may ask questions like, “When did the person get in the car”?,
  from two adjacent frames that has a person outside the car and one
  has the person inside it.  So, the relationship between the frames
  needs to be captured.  Individual questions like where an object is
  in a frame may be answered by scene analysis.  However, it is a
  challenge to ask for relationships between the frames that we plan
  to address.

  In our methodology, we input the video data and identify key frames.
  For each key frame, we do semantic segmentation to get different
  localized objects, which will serve as nodes for the scene
  graph. YOLO and Faster R-CNN are usually used for semantic
  segmentation for their speed and accuracy.  We then use a dense
  captioning algorithm to generate captions for each frame based on
  cite:k2.  Now, we can use generated captions to form a scene
  graph. These scene graphs from captions are generated using the
  algorithm mentioned in cite:s2.  Each node in the scene graph
  corresponds to an object in the current frame and each directed edge
  represents the relationship between the objects.  The next step is
  to link the scene graphs. This step will establish a relation
  between the existing graph and the incoming scene graph from the
  current frame.  We scan the existing graph to append any new nodes
  (objects, attributes) and/or edges (relationships) that come up in
  the new frame.  To answer temporal queries, we add timestamp as an
  edge attribute.  We propose to employ graph alignment techniques and
  explore possible graph similarity measures for detecting scene
  changes.  We may use this to trigger new graph generation between
  distinct scenes.  

  #+CAPTION: Example scene graph cite:s1
  [[./images/scene_graph.png]]

  
  #+CAPTION: Model pipeline 
  [[./images/proposal-pipeline.png]]





* Dataset Details
  The datasets which are considered for running our experiments are
  the Visual Genome and the Youtube-8M databases.  The visual genome
  dataset has 108,077 images which has 75,729 unique objects, 40,480
  unique objects and the number of unique relationships between those
  object, which form the nodes for the respective scene graphs as
  40,513.  

  The second dataset which we will consider for testing our contextual
  question-answering model on videos is the Youtube-8M dataset. The
  dataset contains frame-by-frame annotations for eight million videos
  present.

  We consider these datasets for evaluation of our captioning and
  scene graph generation algorithms.

* Evaluation Criteria
  To evaluate the results, we will primarily rely on human evaluations
  as the baseline.  This can be accomplished using Amazon’s mechanical
  Turk or in-class surveys. This provides a baseline to check the
  accuracy of the generated answers from our question-answering
  model. Once we have this, we have several methods existing for
  evaluating the performance of image based question-answering models
  such as Wu-Palmer Similarity(WUPS) cite:DBLP:conf/cvpr/NohSH16.
  WUPS is a similarity measure between words trained on WordNet. On
  thresholding it, we get different performances.



* Future Work

  Our work can be extended to incorporate speech content of video to
  generate more node edge combinations. This multimodal approach will
  make a denser graph but will store much more contextually rich
  information and can be used to answer much more in-depth questions.
  Once the graph is generated, a description text of the video can be
  generated.  Other attributes of the object can be detected and
  incorporated to answer questions about emotion, expression, logic
  etc. Currently we focus mainly on actions and relationships but our
  work can be extended to emotion and inference based questions.
  Lastly, current video retrieval techniques rely heavily on video
  metadata such as video title/tags/description etc and less on the
  actual content/frames of the video. Extending our work, a video
  retrieval system can search on our representation of videos and
  hence the actual video content.
  


  bibliographystyle:ACM-Reference-Format
  bibliography:manuscript.bib
